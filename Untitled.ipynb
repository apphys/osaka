{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import copy\n",
    "from pdb import set_trace\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from dataloaders import init_dataloaders\n",
    "\n",
    "from MAML.model import ModelConvSynbols, ModelConvOmniglot, ModelConvMiniImagenet, ModelMLPSinusoid\n",
    "from MAML.metalearners import ModelAgnosticMetaLearning, ModularMAML, ProtoMAML\n",
    "from MAML.utils import ToTensor1D, set_seed, is_connected\n",
    "\n",
    "from Utils.bgd_lib.bgd_optimizer import create_BGD_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from omniglot.npy.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from collections import namedtuple\n",
    "import main\n",
    "\n",
    "# Meta dataloaders\n",
    "MetaDLs = namedtuple('MetaDLs', 'train val cl')\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dataset='omniglot',\n",
    "    folder='Data',\n",
    "    num_shots=5, # default=5\n",
    "    num_ways=5,  # default=5\n",
    "    num_shots_test=15, # default=15\n",
    "    batch_size=25,     # default=25\n",
    "    # CL args group\n",
    "    prob_statio=0.98,  # default=0.98\n",
    "    task_sequence=None, # default=None\n",
    "    n_steps_per_task=1, # default=1\n",
    "    use_different_nways=False, # default=False\n",
    ")\n",
    "args.device = torch.device('cuda')\n",
    "\n",
    "meta_dls = MetaDLs(*init_dataloaders(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing models...')\n",
    "wandb = None\n",
    "args.meta_lr = 0.001\n",
    "    \n",
    "def init_models():\n",
    "    model = ModelConvOmniglot(args.num_ways, hidden_size=64, deeper=0)\n",
    "    loss_function = F.cross_entropy\n",
    "\n",
    "    meta_optimizer = torch.optim.Adam(model.parameters(), lr=args.meta_lr)\n",
    "    meta_optimizer_cl = meta_optimizer\n",
    "\n",
    "    metalearner = ModelAgnosticMetaLearning(\n",
    "        model, meta_optimizer, loss_function, args=SimpleNamespace(\n",
    "            device=args.device,\n",
    "            num_ways=args.num_ways,\n",
    "            # Size of the fast adaptation step, ie. learning rate in the gradient descent update.\n",
    "            step_size = 0.1, \n",
    "            is_classification_task = True,\n",
    "             # 'Use the first order approximation, do not use higher-order derivatives during meta-optimization.\n",
    "            first_order = 0,\n",
    "            # for MRCL, freeze all conv layers at cl time\n",
    "            freeze_visual_features = 0,\n",
    "             # number of inner updates\n",
    "            num_steps = 1,  # aka num_adaptation_steps\n",
    "            # Whether or not to learn the (inner loop) step-size. \n",
    "            learn_step_size = False,\n",
    "            # power for update modulation\n",
    "            um_power = 0,\n",
    "            # Whether ot not to learn param specific step-size\n",
    "            per_param_step_size = False,\n",
    "        ))\n",
    "    return metalearner, meta_optimizer, meta_optimizer_cl\n",
    "\n",
    "metalearner, meta_optimizer, meta_optimizer_cl = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/100 [00:00<00:11,  8.79it/s, accuracy=0.3493, inner_loss=1.9088, outer_loss=1.8899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pretraining for 2 epochs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 100/100 [00:03<00:00, 26.59it/s, accuracy=0.8766, inner_loss=1.7413, loss=0.3774]       \n",
      "Training:   1%|          | 1/100 [00:00<00:09, 10.27it/s, accuracy=0.9611, inner_loss=1.7400, outer_loss=0.1557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] results: {'mean_outer_loss': 0.3774388335645198, 'accuracies_after': 0.8765866935253143, 'mean_inner_loss': 1.741299769878387}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 100/100 [00:03<00:00, 26.53it/s, accuracy=0.9094, inner_loss=1.7598, loss=0.2781]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] results: {'mean_outer_loss': 0.27807130031287675, 'accuracies_after': 0.9093600273132324, 'mean_inner_loss': 1.7598116695880888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "num_batches = 100 # Number of batch of tasks per epoch (default: 100)\n",
    "verbose = True\n",
    "patience = 5\n",
    "\n",
    "best_val = 0.\n",
    "epochs_overfitting = 0\n",
    "epoch_desc = 'Epoch {{0: <{0}d}}'.format(1 + int(math.log10(num_epochs)))\n",
    "print(f'\\npretraining for {num_epochs} epochs...\\n')\n",
    "for epoch in range(num_epochs):\n",
    "    metalearner.train(\n",
    "        meta_dls.train,\n",
    "        max_batches=num_batches,\n",
    "        verbose=verbose,\n",
    "        desc='Training',\n",
    "        leave=False)\n",
    "    results = metalearner.evaluate(\n",
    "        meta_dls.val,\n",
    "        max_batches=num_batches,\n",
    "        verbose=verbose,\n",
    "        epoch=epoch,\n",
    "        desc=epoch_desc.format(epoch + 1))\n",
    "    \n",
    "    print(f'[{epoch}] results: {results}')\n",
    "    result_val = results['accuracies_after']\n",
    "    # early stopping:\n",
    "    if best_val < result_val:\n",
    "        epochs_overfitting = 0\n",
    "        best_val = result_val\n",
    "        best_metalearner = copy.deepcopy(metalearner)\n",
    "    else:\n",
    "        epochs_overfitting += 1\n",
    "        if epochs_overfitting > patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model_init = copy.deepcopy(metalearner)\n",
    "del metalearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda/envs/cs330/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/root/anaconda/envs/cs330/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Continual learning for 1 iterations...\n",
      "total Acc: nan, mode accs: ['train_acc=0.00', 'test_acc=nan', 'ood_acc=nan']\tTotal tbd: nan\tit: 0\n",
      "total Acc: 0.90, mode accs: ['train_acc=0.95', 'test_acc=nan', 'ood_acc=0.65']\tTotal tbd: 0.96\tit: 99\n"
     ]
    }
   ],
   "source": [
    "timesteps = 100 # number of timesteps for the CL exp\n",
    "n_runs = 1 # number of runs for CL experiment\n",
    "algo3 = False\n",
    "\n",
    "# TODO: why aren't most of these set within cl_model.__init__?\n",
    "cl_model_init.optimizer_cl = meta_optimizer_cl\n",
    "cl_model_init.cl_strategy = 'loss'\n",
    " # threshold for training on incoming data or not\n",
    "cl_model_init.cl_strategy_thres = 4.0\n",
    "# threshold for task boundary detection (-1 to turn off)\n",
    "cl_model_init.cl_tbd_thres = -1 \n",
    "if 0: # turn off meta-learning at CL time (TODO: wat)\n",
    "    cl_model_init.no_meta_learning = True\n",
    "\n",
    "modes = ['train', 'test', 'ood']\n",
    "is_classification_task = True\n",
    "\n",
    "accuracies = np.zeros([n_runs, timesteps])\n",
    "tbds = np.zeros([n_runs, timesteps])\n",
    "avg_accuracies_mode = dict(zip(modes, [[], [], []]))\n",
    "print(f'\\n Continual learning for {n_runs} iterations...')\n",
    "for run in range(n_runs):\n",
    "    accuracies_mode = dict(zip(modes, [[], [], []]))\n",
    "    \n",
    "    cl_model = copy.deepcopy(cl_model_init)\n",
    "    _, _, meta_optimizer_cl = init_models()\n",
    "    cl_model.optimizer_cl = meta_optimizer_cl\n",
    "    for i, batch in enumerate(meta_dls.cl):\n",
    "        data, labels, task_switch, mode, _, _ = batch\n",
    "        if algo3:\n",
    "            results = cl_model.observe2(batch)\n",
    "        else:\n",
    "            results = cl_model.observe(batch)\n",
    "\n",
    "        # Reporting:\n",
    "        accuracy_after = results[\"accuracy_after\"]\n",
    "        accuracies[run, i] = accuracy_after\n",
    "        accuracies_mode[mode[0]].append(accuracy_after)\n",
    "        tbds[run, i] = float(results['tbd'])\n",
    "\n",
    "        if (verbose and i % 100 == 0) or i == timesteps - 1:\n",
    "            acc = np.mean(accuracies[run, :i])\n",
    "            acc_mode = []\n",
    "            for mode in modes:\n",
    "                acc_mode.append(np.mean(accuracies_mode[mode]))\n",
    "            acc_mode_str = [f'{m}_acc={a:.2f}' for m, a in zip(modes, acc_mode)]\n",
    "            print(f'total Acc: {acc:.2f},', f'mode accs: {acc_mode_str}', end='\\t')\n",
    "            # Note: tbd==task boundary detection\n",
    "            tbd = np.mean(tbds[run, :i])\n",
    "            print(f'Total tbd: {tbd:.2f}', f'it: {i}', sep='\\t')\n",
    "\n",
    "        if i == timesteps - 1:\n",
    "            for mode in modes:\n",
    "                avg_accuracies_mode[mode].append(np.mean(accuracies_mode[mode]))\n",
    "            if run == 0 and is_classification_task:\n",
    "                if acc < 1. / float(args.num_ways) + 0.2:\n",
    "                    print({'fail': 1})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    args, wandb = boilerplate(args)\n",
    "\n",
    "    print('Initializing dataloaders...')\n",
    "    # Notes:\n",
    "    # - the train/val meta dls yield batches of {'train', 'test', 'ways', 'shots_tr', 'shots_te'}\n",
    "    # - the cl meta dl yields list of 6 tensors (TODO?)\n",
    "    meta_train_dataloader, meta_val_dataloader, cl_dataloader = init_dataloaders(args)\n",
    "\n",
    "\n",
    "\n",
    "    print('Executing pretraining...')\n",
    "    cl_model_init = pretraining(\n",
    "        args,\n",
    "        wandb,\n",
    "        metalearner,\n",
    "        meta_train_dataloader,\n",
    "        meta_val_dataloader)\n",
    "\n",
    "    print('Executing continual learning...')\n",
    "    continual_learning(\n",
    "        args,\n",
    "        wandb,\n",
    "        cl_model_init,\n",
    "        meta_optimizer_cl,\n",
    "        cl_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
